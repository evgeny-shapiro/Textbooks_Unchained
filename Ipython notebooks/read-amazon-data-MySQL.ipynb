{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">\n",
    "<font color='blue'>\n",
    "Follows tries in  read-amazon-data-tries-and-sqlite.ipynb , but here for real anr in MySQL\n",
    "</font>\n",
    "\n",
    "<font color='putple'>\n",
    "Tutorial on MySQL in python is @ http://zetcode.com/db/mysqlpython/\n",
    "</font>\n",
    "</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "<font color='magenta'>\n",
    "make database of amazon metadata -- FILTERING BY SUBJECT -- 2runs \n",
    "</font>\n",
    "</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MySQLdb as mdb\n",
    "import sys\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_mo = set([\n",
    " 'dynamics',\n",
    " 'electricity',\n",
    " 'electromagnetic',\n",
    " 'fourier',\n",
    " 'hamiltonans',\n",
    " 'hamiltonian',\n",
    " 'lagrangian',\n",
    " 'lagrangians',\n",
    " 'laser',\n",
    " 'lasers',\n",
    " \"maxwell's\",\n",
    " 'mechanics',\n",
    " 'nonrelativistic',\n",
    " 'optical',\n",
    " 'optics',\n",
    " 'physics',\n",
    " 'quantum',\n",
    " 'radiation',\n",
    " 'relativistic',\n",
    " 'schroedinger',\n",
    " 'spectra',\n",
    " 'spectroscopic',\n",
    " 'spectroscopy'\n",
    "])\n",
    "\n",
    "topics_compact = set([\n",
    " 'hamiltonans',\n",
    " 'hamiltonian',\n",
    " 'lagrangian',\n",
    " 'lagrangians',\n",
    " 'mechanics',\n",
    " 'nonrelativistic',\n",
    " 'relativistic',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def titleFits_byTitle(title,topic_set):\n",
    "    tokens = nltk.word_tokenize(title.lower())\n",
    "    return len(topic_set.intersection(tokens)) > 0\n",
    "\n",
    "def titleFits_byAsin(asin,relevant_asin_set):\n",
    "    return (asin in relevant_asin_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "con_f = mdb.connect('localhost', 'insightUser', 'insight15', 'amazon_filtered_compact');\n",
    "#con_f = mdb.connect('localhost', 'insightUser', 'insight15', 'amazon_metadata_filtered');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_f = con_f.cursor()\n",
    "cur_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--DEBUG--\n",
    "con_f.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_f.execute(\"SELECT VERSION()\")\n",
    "ver = cur_f.fetchone()\n",
    "print \"datavase version\", ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cur_f.execute(\"DROP TABLE IF EXISTS metadata\")\n",
    "    \n",
    "cur_f.execute(\"CREATE TABLE metadata(\\\n",
    "Id INT PRIMARY KEY AUTO_INCREMENT, \\\n",
    "asin VARCHAR(10), \\\n",
    "title VARCHAR(300),\\\n",
    "imURL VARCHAR(200),\\\n",
    "relevant_asins VARCHAR(550),\\\n",
    "salesRank INT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_f.execute(\"SELECT DATABASE()\")\n",
    "print cur_f.fetchall()\n",
    "\n",
    "cur_f.execute(\"SHOW TABLES\")\n",
    "print cur_f.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "defSalesRank = -1\n",
    "defUrl = 'https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcQVFwukzczimNzMwQwt9lRzCtarHKPiDffhfrx1Wm09QZjPFUOg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INSERTING ALTOGETHER while filtering PASS 1\n",
    "\n",
    "i=-1\n",
    "i_failed = 0\n",
    "i_passed = 0\n",
    "PASS = 1\n",
    "\n",
    "all_rel_set = set([])\n",
    "gathered_set = set([])\n",
    "\n",
    "with open('meta_Books.json','r') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        i+=1\n",
    "        if (i%10000 ==0): print \"line \", i\n",
    "        \n",
    "        #DEBUG\n",
    "        #if (i>100): break\n",
    "                        \n",
    "        # PASS 1           \n",
    "        try:\n",
    "            line_dict = eval(line)\n",
    "        \n",
    "            #CHOOSE APPROPIATRE titleFits FOR PASS=1 OR PASS=2\n",
    "            if not titleFits_byTitle(line_dict['title'],topics_compact): continue\n",
    "            \n",
    "            url_str = line_dict['imUrl']  if 'imUrl' in line_dict else defUrl\n",
    "            salesRank = int(line_dict['salesRank']['Books'])  if 'salesRank' in line_dict else defSalesRank\n",
    "\n",
    "            # find a better way to replace a single quote\n",
    "            title_str = line_dict['title'].replace(\"'\",\"\").replace(\"&amp\",\"\")\n",
    "            \n",
    "            all_rel_set.add(line_dict['asin'])\n",
    "            gathered_set.add(line_dict['asin'])\n",
    "        \n",
    "            # figure other relevant books\n",
    "            rel_str = ''\n",
    "            \n",
    "            #if len(rel_str) > 500: break\n",
    "            \n",
    "            if 'related' in line_dict:\n",
    "                for key in line_dict['related']:\n",
    "                    vals = []\n",
    "                    for val in line_dict['related'][key]:\n",
    "                        rel_str += val\n",
    "                        rel_str += ' '\n",
    "                        all_rel_set.add(val) \n",
    "\n",
    "            all_insert =  \"INSERT INTO metadata(asin,title,imUrl,relevant_asins,salesrank) \\\n",
    "                            VALUES('%s','%s','%s','%s','%d')\"\\\n",
    "                            % (line_dict['asin'],\\\n",
    "                               title_str,\\\n",
    "                               url_str,\\\n",
    "                               rel_str,\\\n",
    "                               salesRank)\n",
    "\n",
    "            cur_f.execute(all_insert)\n",
    "            con_f.commit()\n",
    "            i_passed +=1\n",
    "            \n",
    "            if (i_passed%10 ==0): print \"passed: \", i_passed\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            i_failed += 1\n",
    "            if (i_failed%100 ==0): print \"    failed \", i_failed, \"  out of\", i\n",
    "            # print \"cant parse line \", i\n",
    "            # print line\n",
    "            print i, \" -- \", e\n",
    "            print\n",
    "        \n",
    "        if (i_passed > 2000): break\n",
    "            \n",
    "            #if i_failed > 2: break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INSERTING ALTOGETHER while filtering PASS 2\n",
    "\n",
    "i=-1\n",
    "i_failed = 0\n",
    "i_passed = 0\n",
    "PASS = 2\n",
    "\n",
    "gathered_set = set([])\n",
    "\n",
    "with open('meta_Books.json','r') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        i+=1\n",
    "        if (i%10000 ==0): print \"line \", i\n",
    "        \n",
    "        #DEBUG\n",
    "        #if (i>100): break\n",
    "                        \n",
    "            \n",
    "        try:\n",
    "            line_dict = eval(line)\n",
    "            \n",
    "            asin = line_dict['asin']\n",
    "        \n",
    "            #CHOOSE APPROPIATRE titleFits FOR PASS=1 OR PASS=2\n",
    "            #if not titleFits_byTitle(line_dict['title'],topics_compact): continue\n",
    "            if not titleFits_byAsin(asin,all_rel_set_copy): continue\n",
    "            \n",
    "            url_str = line_dict['imUrl']  if 'imUrl' in line_dict else defUrl\n",
    "            salesRank = int(line_dict['salesRank']['Books'])  if 'salesRank' in line_dict else defSalesRank\n",
    "\n",
    "            # find a better way to replace a single quote\n",
    "            title_str = line_dict['title'].replace(\"'\",\"\").replace(\"&amp\",\"\")\n",
    "            \n",
    "            #all_rel_set.add(line_dict['asin'])\n",
    "            gathered_set.add(asin)\n",
    "        \n",
    "            # figure other relevant books\n",
    "            rel_str = ''\n",
    "            \n",
    "            #if len(rel_str) > 500: break\n",
    "            \n",
    "            if 'related' in line_dict:\n",
    "                for key in line_dict['related']:\n",
    "                    vals = []\n",
    "                    for val in line_dict['related'][key]:\n",
    "                        rel_str += val\n",
    "                        rel_str += ' '\n",
    "                        #all_rel_set.add(val) \n",
    "\n",
    "            all_insert =  \"INSERT INTO metadata(asin,title,imUrl,relevant_asins,salesrank) \\\n",
    "                            VALUES('%s','%s','%s','%s','%d')\"\\\n",
    "                            % (asin,\\\n",
    "                               title_str,\\\n",
    "                               url_str,\\\n",
    "                               rel_str,\\\n",
    "                               salesRank)\n",
    "\n",
    "            cur_f.execute(all_insert)\n",
    "            con_f.commit()\n",
    "            i_passed +=1\n",
    "            \n",
    "            if (i_passed%10 ==0): print \"passed: \", i_passed\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            i_failed += 1\n",
    "            if (i_failed%100 ==0): print \"    failed \", i_failed, \"  out of\", i\n",
    "            # print \"cant parse line \", i\n",
    "            # print line\n",
    "            print i, \" -- \", e\n",
    "            print\n",
    "        \n",
    "        #if (i_passed > 2000): break\n",
    "            \n",
    "            #if i_failed > 2: break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print i\n",
    "\n",
    "print i_passed\n",
    "\n",
    "print i_failed\n",
    "\n",
    "print len(all_rel_set_copy)\n",
    "\n",
    "print len(gathered_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_f.execute(\"SELECT asin FROM metadata\")\n",
    "\n",
    "\n",
    "asins = []\n",
    "for asin in cur_f.fetchall():\n",
    "    asins.append(asin[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(asins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle after the first pass\n",
    "import pickle\n",
    "\n",
    "#with open('relevant_asins_1pass.pkl','w') as f:\n",
    "#    pickle.dump(all_rel_set,f)\n",
    "    \n",
    "with open('gathered_asins_2pass.pkl','w') as f:\n",
    "    pickle.dump(asins,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "<font color='red'>\n",
    "make database of amazon metadata -- FILTERING BY SUBJECT -- tries\n",
    "</font>\n",
    "</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MySQLdb as mdb\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_mo = set([\n",
    " 'dynamics',\n",
    " 'electricity',\n",
    " 'electromagnetic',\n",
    " 'fourier',\n",
    " 'hamiltonans',\n",
    " 'hamiltonian',\n",
    " 'lagrangian',\n",
    " 'lagrangians',\n",
    " 'laser',\n",
    " 'lasers',\n",
    " \"maxwell's\",\n",
    " 'mechanics',\n",
    " 'nonrelativistic',\n",
    " 'optical',\n",
    " 'optics',\n",
    " 'physics',\n",
    " 'quantum',\n",
    " 'radiation',\n",
    " 'relativistic',\n",
    " 'schroedinger',\n",
    " 'spectra',\n",
    " 'spectroscopic',\n",
    " 'spectroscopy'\n",
    "])\n",
    "\n",
    "topics_compact = set([\n",
    " 'dynamics',\n",
    " 'hamiltonans',\n",
    " 'hamiltonian',\n",
    " 'lagrangian',\n",
    " 'lagrangians',\n",
    " 'mechanics',\n",
    " 'nonrelativistic',\n",
    " 'relativistic',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics_amo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def titleFits(title,topic_set):\n",
    "    tokens = nltk.word_tokenize(title.lower())\n",
    "    return len(topic_set.intersection(tokens)) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con_f = mdb.connect('localhost', 'insightUser', 'insight15', 'amazon_metadata_filtered');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "con_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_f = con_f.cursor()\n",
    "cur_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_f.execute(\"SELECT VERSION()\")\n",
    "ver = cur_f.fetchone()\n",
    "print \"datavase version\", ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_f.execute(\"DROP TABLE IF EXISTS amazon_metadata_FC\")\n",
    "    \n",
    "cur_f.execute(\"CREATE TABLE amazon_metadata_FC(\\\n",
    "Id INT PRIMARY KEY AUTO_INCREMENT, \\\n",
    "asin VARCHAR(10), \\\n",
    "title VARCHAR(300),\\\n",
    "imURL VARCHAR(200),\\\n",
    "relevant_asins VARCHAR(550),\\\n",
    "salesRank INT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "defSalesRank = -1\n",
    "defUrl = 'https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcQVFwukzczimNzMwQwt9lRzCtarHKPiDffhfrx1Wm09QZjPFUOg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def titleFits(title,topic_set):\n",
    "    tokens = nltk.word_tokenize(title.lower())\n",
    "    return len(topic_set.intersection(tokens)) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INSERTING ALTOGETHER while filtering\n",
    "\n",
    "i=-1\n",
    "i_failed = 0\n",
    "i_passed = 0\n",
    "\n",
    "all_rel_set = set([])\n",
    "\n",
    "with open('meta_Books.json','r') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        i+=1\n",
    "        if (i%10000 ==0): print \"line \", i\n",
    "            \n",
    "        try:\n",
    "            line_dict = eval(line)\n",
    "        \n",
    "            #UNCOMMENT THIS BEFORE THE GRAND RUN\n",
    "            if not titleFits(line_dict['title'],topics_mo): continue\n",
    "            \n",
    "            url_str = line_dict['imUrl']  if 'imUrl' in line_dict else defUrl\n",
    "            salesRank = int(line_dict['salesRank']['Books'])  if 'salesRank' in line_dict else defSalesRank\n",
    "\n",
    "            # find a better way to replace a single quote\n",
    "            title_str = line_dict['title'].replace(\"'\",\"\").replace(\"&amp\",\"_amprsnd_\")\n",
    "            \n",
    "            all_rel_set.add(line_dict['asin'])\n",
    "        \n",
    "            # figure other relevant books\n",
    "            rel_str = ''\n",
    "            \n",
    "            #if len(rel_str) > 500: break\n",
    "            \n",
    "            if 'related' in line_dict:\n",
    "                for key in line_dict['related']:\n",
    "                    vals = []\n",
    "                    for val in line_dict['related'][key]:\n",
    "                        rel_str += val\n",
    "                        rel_str += ' '\n",
    "                        all_rel_set.add(val) \n",
    "\n",
    "            all_insert =  \"INSERT INTO amazon_metadata_filtered(asin,title,imUrl,relevant_asins,salesrank) \\\n",
    "                            VALUES('%s','%s','%s','%s','%d')\"\\\n",
    "                            % (line_dict['asin'],\\\n",
    "                               title_str,\\\n",
    "                               url_str,\\\n",
    "                               rel_str,\\\n",
    "                               salesRank)\n",
    "\n",
    "            cur_f.execute(all_insert)\n",
    "            con_f.commit()\n",
    "            i_passed +=1\n",
    "            if (i_passed%100 ==0): print \"passed: \", i_passed\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            i_failed += 1\n",
    "            if (i_failed%100 ==0): print \"    failed \", i_failed, \"  out of\", i\n",
    "            print \"cant parse line \", i\n",
    "            print line\n",
    "            print e\n",
    "            print\n",
    "            \n",
    "            #if i_failed > 2: break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line_dict['title']\n",
    "\n",
    "s = \"The rogue of publishers' row;: Confessions of a publisher (A Banner Book)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_f.execute(\"SELECT * FROM amazon_metadata_filtered\")\n",
    "print cur_f.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_rel_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7e613b0cb610>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'relevant_asins_reviews.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_rel_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'all_rel_set' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('relevant_asins.pkl','w') as f:\n",
    "    pickle.dump(all_rel_set,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls\n",
    "import pickle\n",
    "with open('relevant_asins_1pass.pkl','rb') as f:\n",
    "    all_rel_set_copy = pickle.load(f)\n",
    "    \n",
    "len(all_rel_set_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "<font color='red'>\n",
    "--------- make database of amazon REVIEWS ----- FILTERING BY SUBJECT -- tries\n",
    "</font>\n",
    "</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MySQLdb as mdb\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "con_f = mdb.connect('localhost', 'insightUser', 'insight15', 'amazon_reviews_filtered');\n",
    "cur_f = con_f.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_f.execute(\"SELECT VERSION()\")\n",
    "ver = cur_f.fetchone()\n",
    "print \"database version\", ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_f.execute(\"DROP TABLE IF EXISTS amazon_reviews_filtered\")\n",
    "    \n",
    "cur_f.execute(\"CREATE TABLE amazon_reviews_filtered(\\\n",
    "Id INT PRIMARY KEY AUTO_INCREMENT, \\\n",
    "asin VARCHAR(10), \\\n",
    "helpful1 INT,\\\n",
    "helpful2 INT,\\\n",
    "overall FLOAT, \\\n",
    "reviewText VARCHAR(10000),\\\n",
    "reviewerID VARCHAR(18),\\\n",
    "reviewSummary VARCHAR(500))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INSERTING one while filtering\n",
    "\n",
    "i=-1\n",
    "i_failed = 0\n",
    "i_passed = 0\n",
    "RUN = 1\n",
    "\n",
    "all_rel_set = set([])\n",
    "\n",
    "with open('reviews_Books_100lines.json','r') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        i+=1\n",
    "        if (i%40 ==0): print \"line \", i\n",
    "            \n",
    "        try:\n",
    "            line_dict = eval(line)\n",
    "        \n",
    "            #UNCOMMENT THIS BEFORE THE GRAND RUN\n",
    "            #if not line_dict['asin'] in all_rel_set: continue\n",
    "                \n",
    "            review_str = line_dict['reviewText'].replace(\"'\",\"\")\n",
    "            summary_str = line_dict['summary'].replace(\"'\",\"\") if 'summary' in line_dict else ''\n",
    "\n",
    "            all_insert_r =  \"INSERT INTO amazon_reviews_filtered\\\n",
    "            (asin, \\\n",
    "            helpful1, \\\n",
    "            helpful2,\\\n",
    "            overall,\\\n",
    "            reviewText,\\\n",
    "            reviewerID,\\\n",
    "            reviewSummary\\\n",
    "            ) \\\n",
    "            VALUES('%s','%d','%d','%f','%s','%s','%s')\" % \\\n",
    "            (line_dict['asin'],\\\n",
    "             line_dict['helpful'][0],\\\n",
    "             line_dict['helpful'][1],\\\n",
    "             line_dict['overall'],\\\n",
    "             review_str,\\\n",
    "             line_dict['reviewerID'],\\\n",
    "             summary_str\n",
    "           )\n",
    "\n",
    "            cur_f.execute(all_insert_r)\n",
    "            con_f.commit()\n",
    "            i_passed +=1\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            i_failed += 1\n",
    "            if (i_failed%100 ==0): print \"    failed \", i_failed, \"  out of\", i\n",
    "            print \"cant parse line \", i\n",
    "            print line\n",
    "            print e\n",
    "            print\n",
    "            \n",
    "            \n",
    "            #if i_failed > 2: break\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print line_dict['asin']\n",
    "print line_dict['helpful'][0]\n",
    "print line_dict['helpful'][1]\n",
    "print line_dict['overall']\n",
    "print review_str\n",
    "print line_dict['reviewerID']\n",
    "print line_dict['summary']\n",
    "print line_dict['reviewTime']\n",
    "print line_dict['unixReviewTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "<font color='magenta'>\n",
    "make database of amazon REVIEWS -- filtering by books -- 2runs \n",
    "</font>\n",
    "</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MySQLdb as mdb\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_metadata_100lines.db\t  read-amazon-data-MySQL.ipynb\r\n",
      "amazon_metadata_phys.db\t\t  read-amazon-data-tries-and-sqlite.ipynb\r\n",
      "db_to_pandas_reviews_tries.ipynb  relevant_asins_1pass.pkl\r\n",
      "gathered_asins_1pass.pkl\t  relevant_asins.pkl\r\n",
      "gathered_asins_2pass.pkl\t  reviews_Books_100lines.json\r\n",
      "meta_Books_100lines.json\t  reviews_Books.json\r\n",
      "meta_Books.json\t\t\t  trycurl.json\r\n",
      "ol_dump_200lines.txt\t\t  Z-INSIGHT-DATA.lnk\r\n",
      "ol_dump_2015-08-31.txt\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10010"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls\n",
    "import pickle\n",
    "with open('gathered_asins_2pass.pkl','rb') as f:\n",
    "    rel_asins = pickle.load(f)\n",
    "    \n",
    "len(rel_asins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def titleFits_byAsin(asin,relevant_asin_set):\n",
    "    return (asin in relevant_asin_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MySQLdb.cursors.Cursor at 0x7fb1380177d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_f = mdb.connect('localhost', 'insightUser', 'insight15', 'amazon_filtered_compact');\n",
    "#con_f = mdb.connect('localhost', 'insightUser', 'insight15', 'amazon_metadata_filtered');\n",
    "\n",
    "cur_f = con_f.cursor()\n",
    "cur_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0L"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_f.execute(\"DROP TABLE IF EXISTS reviews\")\n",
    "    \n",
    "cur_f.execute(\"CREATE TABLE reviews(\\\n",
    "Id INT PRIMARY KEY AUTO_INCREMENT, \\\n",
    "asin VARCHAR(10), \\\n",
    "helpful1 INT,\\\n",
    "helpful2 INT,\\\n",
    "overall FLOAT, \\\n",
    "reviewText VARCHAR(10000),\\\n",
    "reviewerID VARCHAR(18),\\\n",
    "reviewSummary VARCHAR(500))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('amazon_filtered_compact',),)\n",
      "(('metadata',), ('metadata_1pass',), ('reviews',), ('reviews_100lines',))\n"
     ]
    }
   ],
   "source": [
    "cur_f.execute(\"SELECT DATABASE()\")\n",
    "print cur_f.fetchall()\n",
    "\n",
    "cur_f.execute(\"SHOW TABLES\")\n",
    "print cur_f.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line  0\n",
      "line  10000\n",
      "line  20000\n",
      "line  30000\n",
      "line  40000\n",
      " passed  100\n",
      "line  50000\n",
      "line  60000\n",
      "line  70000\n",
      "line  80000\n",
      "line  90000\n",
      "line  100000\n",
      "line  110000\n",
      "line "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:27: Warning: Data truncated for column 'reviewText' at row 1\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:27: Warning: Data truncated for column 'reviewerID' at row 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 120000\n",
      " passed  200\n",
      "line  130000\n",
      " passed  300\n",
      "line  140000\n",
      " passed  400\n",
      " passed  500\n",
      " passed  600\n",
      "line  150000\n",
      " passed  700\n",
      " passed  800\n",
      " passed  900\n",
      " passed  1000\n",
      "line  160000\n",
      "line  170000\n",
      "line  180000\n",
      " passed  1100\n",
      "line  190000\n",
      "line  200000\n",
      "line  210000\n",
      "line  220000\n",
      "line  230000\n",
      " passed  1200\n",
      "line  240000\n",
      " passed  1300\n",
      " passed  1400\n",
      "line  250000\n",
      "line  260000\n",
      "line  270000\n",
      "line  280000\n",
      "line  290000\n",
      " passed  1500\n",
      " passed  1600\n",
      " passed  1700\n",
      " passed  1800\n",
      " passed  1900\n",
      " passed  2000\n",
      " passed  2100\n",
      " passed  2200\n",
      "line  300000\n",
      "line  310000\n",
      " passed  2300\n",
      " passed  2400\n",
      " passed  2500\n",
      "line  320000\n",
      "line  330000\n",
      "line  340000\n",
      " passed  2600\n",
      " passed  2700\n",
      " passed  2800\n",
      "line  350000\n",
      " passed  2900\n",
      "line  360000\n",
      "line  370000\n",
      "line  380000\n",
      "line  390000\n",
      "line  400000\n",
      "line  410000\n",
      "line  420000\n",
      " passed  3000\n",
      " passed  3100\n",
      "line  430000\n",
      "line  440000\n",
      " passed  3200\n",
      "line  450000\n",
      " passed  3300\n",
      " passed  3400\n",
      " passed  3500\n",
      " passed  3600\n",
      "line  460000\n",
      "line  470000\n",
      "line  480000\n",
      "line  490000\n",
      "line  500000\n",
      " passed  3700\n",
      " passed  3800\n",
      " passed  3900\n",
      " passed  4000\n",
      " passed  4100\n",
      " passed  4200\n",
      " passed  4300\n",
      " passed  4400\n",
      " passed  4500\n",
      " passed  4600\n",
      " passed  4700\n",
      " passed  4800\n",
      " passed  4900\n",
      " passed  5000\n",
      " passed  5100\n",
      " passed  5200\n",
      " passed  5300\n",
      " passed  5400\n",
      " passed  5500\n",
      " passed  5600\n",
      " passed  5700\n",
      " passed  5800\n",
      " passed  5900\n",
      " passed  6000\n",
      " passed  6100\n",
      " passed  6200\n",
      " passed  6300\n",
      " passed  6400\n",
      " passed  6500\n",
      " passed  6600\n",
      " passed  6700\n",
      " passed  6800\n",
      " passed  6900\n",
      " passed  7000\n",
      " passed  7100\n",
      " passed  7200\n",
      "line  510000\n",
      "line  520000\n",
      " passed  7300\n",
      "line  530000\n",
      " passed  7400\n",
      "line  540000\n",
      " passed  7500\n",
      "line  550000\n",
      "line  560000\n",
      "line  570000\n",
      "line  580000\n",
      "line  590000\n",
      " passed  7600\n",
      "line  600000\n",
      "line  610000\n",
      "line  620000\n",
      "line  630000\n",
      "line  640000\n",
      "line  650000\n",
      " passed  7700\n",
      " passed  7800\n",
      "line  660000\n",
      "line  670000\n",
      "line  680000\n",
      " passed  7900\n",
      " passed  8000\n",
      "line  690000\n",
      "line  700000\n",
      " passed  8100\n",
      "line  710000\n",
      "line  720000\n",
      "line  730000\n",
      "line  740000\n",
      "line  750000\n",
      "line  760000\n",
      "line  770000\n",
      "line  780000\n",
      "line  790000\n",
      "line  800000\n",
      "line  810000\n",
      "line  820000\n",
      "line  830000\n",
      "line  840000\n",
      "line  850000\n",
      "line  860000\n",
      " passed  8200\n",
      "line  870000\n",
      " passed  8300\n",
      "line  880000\n",
      "line  890000\n",
      " passed  8400\n",
      " passed  8500\n",
      "line  900000\n",
      "line  910000\n",
      " passed  8600\n",
      "line  920000\n",
      "line  930000\n",
      " passed  8700\n",
      " passed  8800\n",
      " passed  8900\n",
      " passed  9000\n",
      " passed  9100\n",
      " passed  9200\n",
      " passed  9300\n",
      " passed  9400\n",
      " passed  9500\n",
      " passed  9600\n",
      " passed  9700\n",
      " passed  9800\n",
      " passed  9900\n",
      " passed  10000\n",
      " passed  10100\n",
      " passed  10200\n",
      " passed  10300\n",
      "line  940000\n",
      "line  950000\n",
      " passed  10400\n",
      "line  960000\n",
      "line  970000\n",
      " passed  10500\n",
      "line  980000\n",
      " passed  10600\n",
      " passed  10700\n",
      " passed  10800\n",
      " passed  10900\n",
      "line  990000\n",
      "line  1000000\n",
      " passed  11000\n",
      " passed  11100\n",
      "line  1010000\n",
      "line  1020000\n",
      " passed  11200\n",
      " passed  11300\n",
      "line  1030000\n",
      " passed  11400\n",
      " passed  11500\n",
      " passed  11600\n",
      " passed  11700\n",
      " passed  11800\n",
      " passed  11900\n",
      " passed  12000\n",
      "line  1040000\n",
      " passed  12100\n",
      " passed  12200\n",
      "line  1050000\n",
      " passed  12300\n",
      " passed  12400\n",
      "line  1060000\n",
      " passed  12500\n",
      " passed  12600\n",
      " passed  12700\n",
      " passed  12800\n",
      " passed  12900\n",
      " passed  13000\n",
      " passed  13100\n",
      " passed  13200\n",
      " passed  13300\n",
      " passed  13400\n",
      "line  1070000\n",
      " passed  13500\n",
      " passed  13600\n",
      " passed  13700\n",
      "line  1080000\n",
      " passed  13800\n",
      " passed  13900\n",
      " passed  14000\n",
      " passed  14100\n",
      " passed  14200\n",
      " passed  14300\n",
      "line  1090000\n",
      " passed  14400\n",
      " passed  14500\n",
      " passed  14600\n",
      " passed  14700\n",
      " passed  14800\n",
      "line  1100000\n",
      " passed  14900\n",
      " passed  15000\n",
      " passed  15100\n",
      "line  1110000\n",
      " passed  15200\n",
      " passed  15300\n",
      " passed  15400\n",
      " passed  15500\n",
      "line  1120000\n",
      " passed  15600\n",
      " passed  15700\n",
      " passed  15800\n",
      " passed  15900\n",
      "line  1130000\n",
      " passed  16000\n",
      " passed  16100\n",
      " passed  16200\n",
      " passed  16300\n",
      " passed  16400\n",
      " passed  16500\n",
      "line  1140000\n",
      " passed  16600\n",
      " passed  16700\n",
      " passed  16800\n",
      " passed  16900\n",
      " passed  17000\n",
      " passed  17100\n",
      "line  1150000\n",
      " passed  17200\n",
      " passed  17300\n",
      " passed  17400\n",
      "line  1160000\n",
      " passed  17500\n",
      " passed  17600\n",
      " passed  17700\n",
      " passed  17800\n",
      "line  1170000\n",
      " passed  17900\n",
      " passed  18000\n",
      " passed  18100\n",
      " passed  18200\n",
      " passed  18300\n",
      " passed  18400\n",
      " passed  18500\n",
      " passed  18600\n",
      " passed  18700\n",
      " passed  18800\n",
      "line  1180000\n",
      " passed  18900\n",
      " passed  19000\n",
      " passed  19100\n",
      " passed  19200\n",
      " passed  19300\n",
      " passed  19400\n",
      "line  1190000\n",
      " passed  19500\n",
      " passed  19600\n",
      " passed  19700\n",
      " passed  19800\n",
      " passed  19900\n",
      " passed  20000\n",
      "line  1200000\n",
      "line  1210000\n",
      "line  1220000\n",
      "line  1230000\n",
      "line  1240000\n",
      "line  1250000\n",
      "line  1260000\n",
      " passed  20100\n",
      " passed  20200\n",
      " passed  20300\n",
      "line  1270000\n",
      " passed  20400\n",
      " passed  20500\n",
      " passed  20600\n",
      " passed  20700\n",
      " passed  20800\n",
      " passed  20900\n",
      "line  1280000\n",
      " passed  21000\n",
      " passed  21100\n",
      " passed  21200\n",
      " passed  21300\n",
      " passed  21400\n",
      "line  1290000\n",
      " passed  21500\n",
      " passed  21600\n",
      " passed  21700\n",
      " passed  21800\n",
      " passed  21900\n",
      " passed  22000\n",
      " passed  22100\n",
      " passed  22200\n",
      "line  1300000\n",
      " passed  22300\n",
      " passed  22400\n",
      " passed  22500\n",
      " passed  22600\n",
      " passed  22700\n",
      " passed  22800\n",
      "line  1310000\n",
      " passed  22900\n",
      " passed  23000\n",
      " passed  23100\n",
      " passed  23200\n",
      " passed  23300\n",
      " passed  23400\n",
      " passed  23500\n",
      " passed  23600\n",
      " passed  23700\n",
      " passed  23800\n",
      "line  1320000\n",
      " passed  23900\n",
      " passed  24000\n",
      " passed  24100\n",
      " passed  24200\n",
      " passed  24300\n",
      " passed  24400\n",
      " passed  24500\n",
      "line  1330000\n",
      " passed  24600\n",
      " passed  24700\n",
      " passed  24800\n",
      " passed  24900\n",
      " passed  25000\n",
      " passed  25100\n",
      " passed  25200\n",
      "line  1340000\n",
      " passed  25300\n",
      " passed  25400\n",
      " passed  25500\n",
      " passed  25600\n",
      " passed  25700\n",
      " passed  25800\n",
      " passed  25900\n",
      " passed  26000\n",
      " passed  26100\n",
      " passed  26200\n",
      "line  1350000\n",
      " passed  26300\n",
      " passed  26400\n",
      " passed  26500\n",
      " passed  26600\n",
      " passed  26700\n",
      " passed  26800\n",
      "line  1360000\n",
      " passed  26900\n",
      "line  1370000\n",
      "line  1380000\n",
      " passed  27000\n",
      " passed  27100\n",
      " passed  27200\n",
      "line  1390000\n",
      " passed  27300\n",
      " passed  27400\n",
      " passed  27500\n",
      " passed  27600\n",
      " passed  27700\n",
      " passed  27800\n",
      " passed  27900\n",
      " passed  28000\n",
      " passed  28100\n",
      " passed  28200\n",
      " passed  28300\n",
      " passed  28400\n",
      " passed  28500\n",
      " passed  28600\n",
      " passed  28700\n",
      " passed  28800\n",
      " passed  28900\n",
      "line  1400000\n",
      " passed  29000\n",
      "line  1410000\n",
      " passed  29100\n",
      " passed  29200\n",
      "line  1420000\n",
      " passed  29300\n",
      " passed  29400\n",
      " passed  29500\n",
      " passed  29600\n",
      " passed  29700\n",
      "line  1430000\n",
      " passed  29800\n",
      " passed  29900\n",
      "line  1440000\n",
      " passed  30000\n",
      " passed  30100\n",
      " passed  30200\n",
      " passed  30300\n",
      " passed  30400\n",
      " passed  30500\n",
      " passed  30600\n",
      " passed  30700\n",
      " passed  30800\n",
      " passed  30900\n",
      " passed  31000\n",
      "line  1450000\n",
      " passed  31100\n",
      " passed  31200\n",
      "line  1460000\n",
      " passed  31300\n",
      "line  1470000\n",
      "line  1480000\n",
      "line  1490000\n",
      "line  1500000\n",
      "line  1510000\n",
      "line  1520000\n",
      "line  1530000\n",
      " passed  31400\n",
      "line  1540000\n",
      "line  1550000\n",
      "line  1560000\n",
      "line  1570000\n",
      "line  1580000\n",
      " passed  31500\n",
      "line  1590000\n",
      " passed  31600\n",
      "line  1600000\n",
      " passed  31700\n",
      " passed  31800\n",
      "line  1610000\n",
      "line  1620000\n",
      " passed  31900\n",
      " passed  32000\n",
      "line  1630000\n",
      "line  1640000\n",
      " passed  32100\n",
      "line  1650000\n",
      "line  1660000\n",
      "line  1670000\n",
      " passed  32200\n",
      "line  1680000\n",
      "line  1690000\n",
      "line  1700000\n",
      "line  1710000\n",
      " passed  32300\n",
      "line  1720000\n",
      "line  1730000\n",
      "line  1740000\n",
      " passed  32400\n",
      " passed  32500\n",
      "line  1750000\n",
      "line  1760000\n",
      "line  1770000\n",
      "line  1780000\n",
      "line  1790000\n",
      "line  1800000\n",
      "line  1810000\n",
      "line  1820000\n",
      "line  1830000\n",
      " passed  32600\n",
      " passed  32700\n",
      "line  1840000\n",
      " passed  32800\n",
      " passed  32900\n",
      "line  1850000\n",
      " passed  33000\n",
      "line  1860000\n",
      " passed  33100\n",
      "line  1870000\n",
      " passed  33200\n",
      " passed  33300\n",
      " passed  33400\n",
      "line  1880000\n",
      " passed  33500\n",
      "line  1890000\n",
      " passed  33600\n",
      " passed  33700\n",
      " passed  33800\n",
      "line  1900000\n",
      " passed  33900\n",
      " passed  34000\n",
      "line  1910000\n",
      "line  1920000\n",
      " passed  34100\n",
      " passed  34200\n",
      "line  1930000\n",
      " passed  34300\n",
      " passed  34400\n",
      " passed  34500\n",
      " passed  34600\n",
      "line  1940000\n",
      " passed  34700\n",
      " passed  34800\n",
      " passed  34900\n",
      " passed  35000\n",
      " passed  35100\n",
      "line  1950000\n",
      " passed  35200\n",
      " passed  35300\n",
      " passed  35400\n",
      " passed  35500\n",
      " passed  35600\n",
      " passed  35700\n",
      " passed  35800\n",
      "line  1960000\n",
      " passed  35900\n",
      " passed  36000\n",
      " passed  36100\n",
      " passed  36200\n",
      " passed  36300\n",
      " passed  36400\n",
      " passed  36500\n",
      "line  1970000\n",
      " passed  36600\n",
      " passed  36700\n",
      " passed  36800\n",
      " passed  36900\n",
      " passed  37000\n",
      "line  1980000\n",
      " passed  37100\n",
      " passed  37200\n",
      " passed  37300\n",
      " passed  37400\n",
      " passed  37500\n",
      " passed  37600\n",
      " passed  37700\n",
      " passed  37800\n",
      " passed  37900\n",
      " passed  38000\n",
      " passed  38100\n",
      "line  1990000\n",
      " passed  38200\n",
      " passed  38300\n",
      "line  2000000\n",
      " passed  38400\n",
      "line  2010000\n",
      " passed  38500\n",
      " passed  38600\n",
      " passed  38700\n",
      " passed  38800\n",
      " passed  38900\n",
      " passed  39000\n",
      " passed  39100\n",
      " passed  39200\n",
      " passed  39300\n",
      " passed  39400\n",
      "line  2020000\n",
      "line  2030000\n",
      "line  2040000\n",
      " passed  39500\n",
      "line  2050000\n",
      " passed  39600\n",
      "line  2060000\n",
      " passed  39700\n",
      " passed  39800\n",
      " passed  39900\n",
      "line  2070000\n",
      "line  2080000\n",
      " passed  40000\n",
      " passed  40100\n",
      " passed  40200\n",
      " passed  40300\n",
      " passed  40400\n",
      " passed  40500\n",
      "line  2090000\n",
      " passed  40600\n",
      " passed  40700\n",
      " passed  40800\n",
      " passed  40900\n",
      "line  2100000\n",
      "line  2110000\n",
      "line  2120000\n",
      "line  2130000\n",
      "line  2140000\n",
      " passed  41000\n",
      " passed  41100\n",
      "line  2150000\n",
      " passed  41200\n",
      "line  2160000\n",
      " passed  41300\n",
      " passed  41400\n",
      "line  2170000\n",
      "line  2180000\n",
      "line  2190000\n",
      " passed  41500\n",
      " passed  41600\n",
      " passed  41700\n",
      "line  2200000\n",
      "line  2210000\n",
      "line  2220000\n",
      "line  2230000\n",
      "line  2240000\n",
      " passed  41800\n",
      " passed  41900\n",
      " passed  42000\n",
      "line  2250000\n",
      "line  2260000\n",
      "line  2270000\n",
      "line  2280000\n",
      "line  2290000\n",
      "line  2300000\n",
      " passed  42100\n",
      " passed  42200\n",
      "line  2310000\n",
      "line  2320000\n",
      " passed  42300\n",
      "line  2330000\n",
      "line  2340000\n",
      "line  2350000\n",
      " passed  42400\n",
      " passed  42500\n",
      " passed  42600\n",
      " passed  42700\n",
      "line  2360000\n",
      "line  2370000\n",
      "line  2380000\n",
      "line  2390000\n",
      "line  2400000\n",
      "line  2410000\n",
      "line  2420000\n",
      "line  2430000\n",
      "line  2440000\n",
      " passed  42800\n",
      "line  2450000\n",
      "line  2460000\n",
      "line  2470000\n",
      "line  2480000\n",
      "line  2490000\n",
      "line  2500000\n",
      "line  2510000\n",
      " passed  42900\n",
      "line  2520000\n",
      " passed  43000\n",
      " passed  43100\n",
      " passed  43200\n",
      " passed  43300\n",
      " passed  43400\n",
      " passed  43500\n",
      "line  2530000\n",
      "line  2540000\n",
      "line  2550000\n",
      "line  2560000\n",
      "line  2570000\n",
      " passed  43600\n",
      "line  2580000\n",
      " passed  43700\n",
      " passed  43800\n",
      " passed  43900\n",
      " passed  44000\n",
      " passed  44100\n",
      " passed  44200\n",
      "line  2590000\n",
      "line  2600000\n",
      "line  2610000\n",
      " passed  44300\n",
      " passed  44400\n",
      " passed  44500\n",
      " passed  44600\n",
      " passed  44700\n",
      " passed  44800\n",
      " passed  44900\n",
      " passed  45000\n",
      " passed  45100\n",
      " passed  45200\n",
      " passed  45300\n",
      "line  2620000\n",
      "line  2630000\n",
      "line  2640000\n",
      "line  2650000\n",
      "line  2660000\n",
      "line  2670000\n",
      "line  2680000\n",
      "line  2690000\n",
      " passed  45400\n",
      "line  2700000\n",
      "line  2710000\n",
      "line  2720000\n",
      "line  2730000\n",
      " passed  45500\n",
      "line  2740000\n",
      "line  2750000\n",
      " passed  45600\n",
      "line  2760000\n",
      "line  2770000\n",
      " passed  45700\n",
      "line  2780000\n",
      "line  2790000\n",
      "line  2800000\n",
      "line  2810000\n",
      "line  2820000\n",
      "line  2830000\n",
      "line  2840000\n",
      "line  2850000\n",
      "line  2860000\n",
      "line  2870000\n",
      " passed  45800\n",
      " passed  45900\n",
      " passed  46000\n",
      " passed  46100\n",
      " passed  46200\n",
      " passed  46300\n",
      " passed  46400\n",
      " passed  46500\n",
      " passed  46600\n",
      "line  2880000\n",
      " passed  46700\n",
      " passed  46800\n",
      " passed  46900\n",
      " passed  47000\n",
      " passed  47100\n",
      " passed  47200\n",
      "line  2890000\n",
      "line  2900000\n",
      "line  2910000\n",
      " passed  47300\n",
      " passed  47400\n",
      " passed  47500\n",
      " passed  47600\n",
      " passed  47700\n",
      "line  2920000\n",
      "line  2930000\n",
      " passed  47800\n",
      " passed  47900\n",
      " passed  48000\n",
      "line  2940000\n",
      " passed  48100\n",
      "line  2950000\n",
      " passed  48200\n",
      "line  2960000\n",
      "line  2970000\n",
      " passed  48300\n",
      "line  2980000\n",
      "line  2990000\n",
      "line  3000000\n",
      " passed  48400\n",
      " passed  48500\n",
      "line  3010000\n",
      "line  3020000\n",
      "line  3030000\n",
      " passed  48600\n",
      "line  3040000\n",
      "line  3050000\n",
      "line  3060000\n",
      "line  3070000\n",
      "line  3080000\n",
      "line  3090000\n",
      "line  3100000\n",
      "line  3110000\n",
      "line  3120000\n",
      "line  3130000\n",
      "line  3140000\n",
      "line  3150000\n",
      " passed  48700\n",
      "line  3160000\n",
      " passed  48800\n",
      "line  3170000\n",
      "line  3180000\n",
      "line  3190000\n",
      "line  3200000\n",
      " passed  48900\n",
      " passed  49000\n",
      "line  3210000\n",
      "line  3220000\n",
      "line  3230000\n",
      "line  3240000\n",
      "line  3250000\n",
      "line  3260000\n",
      "line  3270000\n",
      "line  3280000\n",
      "line  3290000\n",
      " passed  49100\n",
      "line  3300000\n",
      "line  3310000\n",
      "line  3320000\n",
      "line  3330000\n",
      " passed  49200\n",
      "line  3340000\n",
      " passed  49300\n",
      " passed  49400\n",
      "line  3350000\n",
      "line  3360000\n",
      "line  3370000\n",
      "line  3380000\n",
      "line  3390000\n",
      "line  3400000\n",
      "line  3410000\n",
      "line  3420000\n",
      "line  3430000\n",
      " passed  49500\n",
      " passed  49600\n",
      "line  3440000\n",
      "line  3450000\n",
      " passed  49700\n",
      " passed  49800\n",
      "line  3460000\n",
      "line  3470000\n",
      " passed  49900\n",
      " passed  50000\n",
      " passed  50100\n",
      "line  3480000\n",
      " passed  50200\n",
      " passed  50300\n",
      " passed  50400\n",
      "cant parse line  3485420 EOL while scanning string literal (<string>, line 1)\n"
     ]
    }
   ],
   "source": [
    "# GATHER BASED ON FILTERING BY ASINS\n",
    "\n",
    "i=-1\n",
    "i_failed = 0\n",
    "i_passed = 0\n",
    "\n",
    "gathered_asins = set([])\n",
    "\n",
    "with open('reviews_Books.json','r') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        i+=1\n",
    "        if (i%10000 ==0): print \"line \", i\n",
    "            \n",
    "        try:\n",
    "            line_dict = eval(line)\n",
    "        \n",
    "            #UNCOMMENT THIS BEFORE THE GRAND RUN\n",
    "            if not line_dict['asin'] in rel_asins: continue\n",
    "                \n",
    "            review_str = line_dict['reviewText'].replace(\"'\",\"\")\n",
    "            summary_str = line_dict['summary'].replace(\"'\",\"\") if 'summary' in line_dict else ''\n",
    "\n",
    "            all_insert_r =  \"INSERT INTO reviews\\\n",
    "            (asin, \\\n",
    "            helpful1, \\\n",
    "            helpful2,\\\n",
    "            overall,\\\n",
    "            reviewText,\\\n",
    "            reviewerID,\\\n",
    "            reviewSummary\\\n",
    "            ) \\\n",
    "            VALUES('%s','%d','%d','%f','%s','%s','%s')\" % \\\n",
    "            (line_dict['asin'],\\\n",
    "             line_dict['helpful'][0],\\\n",
    "             line_dict['helpful'][1],\\\n",
    "             line_dict['overall'],\\\n",
    "             review_str,\\\n",
    "             line_dict['reviewerID'],\\\n",
    "             summary_str\n",
    "           )\n",
    "\n",
    "            cur_f.execute(all_insert_r)\n",
    "            con_f.commit()\n",
    "            i_passed +=1\n",
    "            gathered_asins.add(line_dict['asin'])\n",
    "            if (i_passed % 100 ==0): print ' passed ', i_passed\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            i_failed += 1\n",
    "            if (i_failed%100 ==0): print \"    failed \", i_failed, \"  out of\", i\n",
    "            print \"cant parse line \", i, e\n",
    "            #print line\n",
    "            #print e\n",
    "            #print\n",
    "            \n",
    "            \n",
    "            #if i_failed > 2: break\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(gathered_asins)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('relevant_asins_reviews.pkl','w') as f:\n",
    "    pickle.dump(gathered_asins,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0321503031\n",
      "20\n",
      "21\n",
      "5.0\n",
      "Its a good physics textbook. Physics is one of those subjects where you need examples to really understand how it works, and this book has a lot of them. Although the main text is pretty busy and the font tiny, the chapters are logically organized and usually contain a concise summary of their contents. Its a pretty good book.\n",
      "A25PYXY8W8BCZS\n",
      "Move up to creating more complex Flash\n",
      "06 20, 2007\n",
      "1182297600\n"
     ]
    }
   ],
   "source": [
    "print line_dict['asin']\n",
    "print line_dict['helpful'][0]\n",
    "print line_dict['helpful'][1]\n",
    "print line_dict['overall']\n",
    "print review_str\n",
    "print line_dict['reviewerID']\n",
    "print line_dict['summary']\n",
    "print line_dict['reviewTime']\n",
    "print line_dict['unixReviewTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_f.execute(\"SELECT reviewText FROM reviews_100lines\")\n",
    "\n",
    "revs = cur_f.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "revs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "<font color='red'>\n",
    "TRIES\n",
    "</font>\n",
    "</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_relevants(line_dict,all_rel_set):\n",
    "    \"\"\" from a dictionary obtained from an amazon metadata line,\n",
    "    creates the string entry for the 'related' field\n",
    "    and updates the set of related books \"\"\"\n",
    "\n",
    "    if not('related' in line_dict):\n",
    "        return all_rell_set, \"\"\n",
    "\n",
    "    vals = []\n",
    "    for val in relD[key]:\n",
    "        vals.append(val)\n",
    "        rel_str += val\n",
    "        rel_str += ' '\n",
    "        \n",
    "    all_rel_set.update(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INSERTING VALUE BY VALUE - TYOUT - not goes in production\n",
    "\n",
    "i=-1\n",
    "i_failed = 0\n",
    "i_passed = 0\n",
    "\n",
    "all_rel_set = set([])\n",
    "\n",
    "with open('meta_Books_100lines.json','r') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        i+=1\n",
    "        #if (i%10000 ==0): \n",
    "        print \"line \", i\n",
    "        \n",
    "        line_dict = eval(line)\n",
    "                \n",
    "        asin_insert = \"INSERT INTO amazon_metadata_filtered(asin) VALUE('%s')\" \\\n",
    "                        % (line_dict['asin'])\n",
    "        \n",
    "        title_insert = \"INSERT INTO amazon_metadata_filtered(title) VALUES('%s')\" \\\n",
    "                        % (line_dict['title'])\n",
    "            \n",
    "        url_str = line_dict['imUrl']  if 'imUrl' in line_dict else defUrl\n",
    "        \n",
    "        url_insert = \"INSERT INTO amazon_metadata_filtered(imURL) VALUES('%s')\"\\\n",
    "                       % (url_str)\n",
    "            update\n",
    "        salesRank = line_dict['salesRank']['Books']  if 'salesRank' in line_dict else defSalesRank\n",
    "        salesRank_insert = \"INSERT INTO amazon_metadata_filtered(salesrank) VALUES('%d')\" \\\n",
    "                       % (salesRank)\n",
    "            \n",
    "        cur_f.execute(asin_insert)\n",
    "        cur_f.execute(title_insert)\n",
    "        cur_f.execute(url_insert)\n",
    "        cur_f.execute(salesRank_insert)\n",
    "        \n",
    "        # figure other relevant books\n",
    "        \n",
    "        rel_str = ''\n",
    "        \n",
    "        if 'related' in line_dict:\n",
    "            \n",
    "            for key in line_dict['related']:\n",
    "                \n",
    "                vals = []\n",
    "                \n",
    "                for val in line_dict['related'][key]:\n",
    "                    vals.append(val)\n",
    "                    rel_str += val\n",
    "                    rel_str += ' '\n",
    "        \n",
    "                all_rel_set.update(vals) \n",
    "            \n",
    "        rel_insert = \"INSERT INTO amazon_metadata_filtered(relevant_asins) VALUES('%s')\"\\\n",
    "                       % (rel_str)\n",
    "            \n",
    "        cur_f.execute(rel_insert)\n",
    "            \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INSERTING ALTOGETHER while filtering\n",
    "\n",
    "i=-1\n",
    "i_failed = 0\n",
    "i_passed = 0\n",
    "\n",
    "all_rel_set = set([])\n",
    "\n",
    "with open('reviews_Books_100lines.json','r') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        i+=1\n",
    "        if (i%40 ==0): print \"line \", i\n",
    "            \n",
    "        try:\n",
    "            line_dict = eval(line)\n",
    "        \n",
    "            #UNCOMMENT THIS BEFORE THE GRAND RUN\n",
    "            #if not line_dict['asin'] in all_rel_set: continue\n",
    "                \n",
    "            review_str = line_dict['reviewText'].replace(\"'\",\"\")\n",
    "            summary_str = line_dict['summary'] if 'summary' in line_dict else ''\n",
    "\n",
    "            \n",
    "            all_insert_r =  \"INSERT INTO amazon_reviews_filtered\\\n",
    "            (asin, \\\n",
    "            helpful1, \\\n",
    "            helpful2, \\\n",
    "            overall,\\\n",
    "            reviewText,\\\n",
    "            reviewerID,\\\n",
    "            reviewSummary\\\n",
    "            ) \\\n",
    "            VALUES('%s','%d','%d','%f',%s','%s','%s','%s',%d')\" % \\\n",
    "            (line_dict['asin'],\\\n",
    "             line_dict['helpful'][0],\\\n",
    "             line_dict['helpful'][1],\\\n",
    "             line_dict['overall'],\\\n",
    "             review_str,\\\n",
    "             line_dict['reviewerID'],\\\n",
    "             summary_str)\n",
    "\n",
    "            cur_f.execute(all_insert_r)\n",
    "            con_f.commit()\n",
    "            i_passed +=1\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            i_failed += 1\n",
    "            if (i_failed%100 ==0): print \"    failed \", i_failed, \"  out of\", i\n",
    "            print \"cant parse line \", i\n",
    "            print all_insert_r\n",
    "            print \n",
    "            print e\n",
    "            print\n",
    "            print\n",
    "            \n",
    "            break\n",
    "            \n",
    "            #if i_failed > 2: break\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tryset={'ff','reg'}\n",
    "\n",
    "print ('ff' in tryset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
